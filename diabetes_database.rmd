---
title: "Aprendizagem Estatística em Altas Dimensões [MAE0501/MAE5904/IBI5904]"
author: ""| Ícaro Maia Santos de Castro\\thanks{Número USP: 11866921}\n| Rayssa de"
  Carvalho Roberto\\thanks{Número USP: 10940828}\n| Rodrigo Aoyama Nakahara\\thanks{Número
  USP: 3510922} \n| Rodrigo Araujo\\thanks{Número USP: 9299208}\n| Vitor Hugo Vieira
  de Lima\\thanks{Número USP: 10263886}\n"
date: "`r stringr::str_to_sentence(format(Sys.time(), '%B de %Y'))`"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
  pdf_document:
    fig_caption: yes
    toc: yes
    toc_depth: 2
lang: pt-BR
header-includes:
- \usepackage{float}
- \usepackage{amsmath}
- \usepackage{amsthm}
- \floatplacement{figure}{H}
- \usepackage{indentfirst}
- \setlength{\parindent}{4em}
- \setlength{\parskip}{1em}
- \usepackage{booktabs}
- \usepackage{dcolumn}
- \usepackage{bm}
- \usepackage{titling}
- \thanksmarkseries{arabic} % \thanks footnotes com numeros
- \usepackage[bottom]{footmisc} % corrige posição footnotes
- \usepackage{pdfpages}
- \usepackage{tocloft}
- \renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
- \usepackage{amssymb}
- \renewcommand\qedsymbol{$\blacksquare$}
- \usepackage{cleveref}
editor_options:
  chunk_output_type: console
---
  
\pagebreak

\newcommand\invisiblesection[1]{%
  \refstepcounter{section}%
  \addcontentsline{toc}{section}{#1}%
  \sectionmark{#1}}
  
<!-- \newcommand\invisiblesection[1]{% -->
<!--   \refstepcounter{section}% -->
<!--   \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}% -->
<!--   \sectionmark{#1}} -->

<!-- \newcommand\invisiblessection[1]{% -->
<!--   \refstepcounter{subsection}% -->
<!--   \addcontentsline{toc}{subsection}{\protect\numberline{\thesection}#1}% -->
<!--   \sectionmark{#1}} -->

```{r setup, include=FALSE}
# options(tinytex.verbose = TRUE)

# template com paramentros padrao para as figuras
knitr::opts_template$set(figuras = list(echo = FALSE, 
                                        results='hide', 
                                        fig.show = "hold",
                                        fig.align = 'center',
                                        fig.ncol = 1,
                                        # fig.width = 4,
                                        # fig.height = 4,
                                        out.width = "\\textwidth",
                                        out.height = "0.9\\textheight"
))
knitr::opts_template$set(figuras2 = list(echo = FALSE, 
                                         results='hide', 
                                         fig.show = "hold",
                                         fig.align = 'center',
                                         fig.ncol = 2,
                                         fig.width = 4,
                                         out.width = "0.48\\textwidth", 
                                         out.height = "0.30\\textheight"))

knitr::opts_template$set(
  series = list(echo = FALSE, 
                results='hide', 
                fig.show = "hold",
                warning=FALSE,
                fig.align = 'center',
                fig.ncol = 1,
                fig.heigth=3, 
                fig.width=16
                # out.width = "\\textheight"
                ))

# uso: <<r, opts.label='figuras'>>=
# uso: ```{r, opts.label='figuras'}


```

```{r libs, include=FALSE}
library(tidyverse)
library(knitr)
library(stargazer)
library(tidyverse)
library(kableExtra)
library(ggplot2)
library(readxl)
library(MASS)
library(psych)
library(Hmisc)
library(GPArotation)
library(corrplot)


library(dplyr)
library(visdat)
library(VIM)
library(ggplot2)
library(naniar)

library(MLmetrics)
library(ROCR)
library(pROC)

library(modelr)
library(dplyr)


library(vegan)
```

```{r func_aux, include=FALSE}
# Funções auxliares ----

kable <- function(...) {
  args <- list(
    ...,
    {if(!interactive() & knitr::is_latex_output()) format = 'latex'},
    digits = 2,
    booktabs = TRUE,
    format.args = list(
      # scientific=T,
      # nsmall = 2,
      decimal.mark = ",", 
      big.mark = "."
      )
    )
  
  args <- args[!duplicated(names(args))]
  
  do.call(knitr::kable, args) %>% kable_styling(latex_options = "HOLD_position")
}



```




# Importando os dados / Limpando / Inspecionando 



```{r}

diabetes <- read_csv("C:\\Users\\Rodrigo Araujo\\Documents\\IME-USP\\Aprendizagem Estatística em Altas Dimensões\\diabetes-database-mae0501\\diabetes.csv")

#diabetes <- read.csv("diabetes.csv")
head(diabetes) %>% kable(caption="Dados.")

dim(diabetes)

```


## Renomear coluna e ajustar níveis de categórica

```{r}

diabetes[, 2:6][diabetes[, 2:6] == 0] <- NA

colnames(diabetes)[9] <- "diabetes"

diabetes$diabetes <- as.factor(diabetes$diabetes)

levels(diabetes$diabetes) <- c("No","Yes")

```


## Visualização dos Dados {.tabset}

### Estrutura dos Dados

```{r}

str(diabetes)

```


# train / test

```{r, warning=FALSE}

# para reprodução
set.seed(23) 
nrows <- nrow(diabetes)
index <- sample(1:nrows, 0.7 * nrows)	# shuffle and divide
# train <- diab                         # 768 test data (100%)
train <- diabetes[index,]			        # 537 test data (70%)
test <- diabetes[-index,]  		            # 231 test data (30%)


```


## Proporção de diabetes (Benign / Malignant) {.tabset}

### train

```{r}

prop.table(table(train$diabetes))

```

### test

```{r}

prop.table(table(test$diabetes))

```



# Analise dos Missings

## Descritiva inicial

```{r, warning=FALSE}

summary(train)

```



```{r}
vis_dat(train)

```

```{r}
vis_miss(train)

```

```{r}
aggr(train) # Missings têm padrões?

```

```{r}
ggplot(train, aes(x = Insulin, y = SkinThickness)) + # Padrão de missing entre 2 vars com mais missings?
  geom_miss_point()

```

```{r}
ggplot(train, aes(x = Insulin, y = SkinThickness)) + # Padrão de missing entre 2 vars com mais missings por categoria da resposta
  geom_miss_point() + facet_wrap(~ diabetes)

marginplot(diabetes[c(4,5)])

```


# Remoção de NAs
```{r, warning=FALSE}

train_without_NAs <- train[complete.cases(train), ]
dim(train_without_NAs)

test_without_NAs <- test[complete.cases(test), ]
dim(test_without_NAs)

```


## Possibilidades de imputação

```{r}
library(mice)

imputacao_train <- mice(data = train , m = 5, maxit = 50, meth = 'pmm', seed = 25)
summary(imputacao_train)

``` 

```{r}
imputado_train_1 <- complete(imputacao_train, 1)
imputado_train_2 <- complete(imputacao_train, 2)
imputado_train_3 <- complete(imputacao_train, 3)
imputado_train_4 <- complete(imputacao_train, 4)
imputado_train_5 <- complete(imputacao_train, 5)

library(lattice)
densityplot(imputacao_train)

``` 

```{r}
  
train$Glucose <- apply(cbind(imputado_train_1$Glucose, imputado_train_2$Glucose, imputado_train_3$Glucose, imputado_train_4$Glucose, imputado_train_5$Glucose), 1, mean)  

train$BloodPressure <- apply(cbind(imputado_train_1$BloodPressure, imputado_train_2$BloodPressure, imputado_train_3$BloodPressure, imputado_train_4$BloodPressure, imputado_train_5$BloodPressure), 1, mean)  

train$SkinThickness <- apply(cbind(imputado_train_1$SkinThickness, imputado_train_2$SkinThickness, imputado_train_3$SkinThickness, imputado_train_4$SkinThickness, imputado_train_5$SkinThickness), 1, mean)  

train$Insulin <- apply(cbind(imputado_train_1$Insulin, imputado_train_2$Insulin, imputado_train_3$Insulin, imputado_train_4$Insulin, imputado_train_5$Insulin), 1, mean)  

train$BMI <- apply(cbind(imputado_train_1$BMI, imputado_train_2$BMI, imputado_train_3$BMI, imputado_train_4$BMI, imputado_train_5$BMI), 1, mean)  

```



```{r}
imputacao_test <- mice(data = test , m = 5, maxit = 50, meth = 'pmm', seed = 25)
summary(imputacao_test)

```


```{r}
imputado_test_1 <- complete(imputacao_test, 1)
imputado_test_2 <- complete(imputacao_test, 2)
imputado_test_3 <- complete(imputacao_test, 3)
imputado_test_4 <- complete(imputacao_test, 4)
imputado_test_5 <- complete(imputacao_test, 5)

``` 

```{r}
  
test$Glucose <- apply(cbind(imputado_test_1$Glucose, imputado_test_2$Glucose, imputado_test_3$Glucose, imputado_test_4$Glucose, imputado_test_5$Glucose), 1, mean)  

test$BloodPressure <- apply(cbind(imputado_test_1$BloodPressure, imputado_test_2$BloodPressure, imputado_test_3$BloodPressure, imputado_test_4$BloodPressure, imputado_test_5$BloodPressure), 1, mean)  

test$SkinThickness <- apply(cbind(imputado_test_1$SkinThickness, imputado_test_2$SkinThickness, imputado_test_3$SkinThickness, imputado_test_4$SkinThickness, imputado_test_5$SkinThickness), 1, mean)  

test$Insulin <- apply(cbind(imputado_test_1$Insulin, imputado_test_2$Insulin, imputado_test_3$Insulin, imputado_test_4$Insulin, imputado_test_5$Insulin), 1, mean)  

test$BMI <- apply(cbind(imputado_test_1$BMI, imputado_test_2$BMI, imputado_test_3$BMI, imputado_test_4$BMI, imputado_test_5$BMI), 1, mean)  

```





# Análise Descritiva

## Distribuição da variável Diabetes 


```{r, warning=FALSE}

ggplot(train, aes(diabetes, fill = diabetes)) + 
  geom_bar() +
  theme_bw() +
  labs(title = "Classificação Diabetes", x = "Diabetes") +
  theme(plot.title = element_text(hjust = 0.5))

```


## Correlação entre cada variável

```{r, warning=FALSE}

library(PerformanceAnalytics)

chart.Correlation(train[,-9], histogram=TRUE, col="grey10", pch=1, main="Correlação entre ás variáveis explicativas")

```



```{r, warning=FALSE}
library(ggcorrplot)

corr<-round(cor(train[,-9]),1)
ggcorrplot(corr, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("red", "white", "blue"), 
           title="Correlogram of Diabetes data", 
           ggtheme=theme_bw)

```






# Modelagem

## SVM

```{r, warning=FALSE}
library(caret)
library(e1071)

set.seed(123)

linear.tune <- e1071::tune.svm(diabetes ~.,
                                data   = train,
                                kernel = 'linear',
                                cost   = c(0.001, 0.01, 0.1, 1, 5, 10))
summary(linear.tune)

```

Matriz de Confusão

```{r, warning=FALSE}

svm.real <- test$diabetes

best.linear <- linear.tune$best.model
tune.test <- predict(best.linear, test[,-9])

caret::confusionMatrix(data = tune.test,
                       reference = svm.real,
                       positive  = 'Yes')

```

Curva ROC

```{r, warning=FALSE}

svm.predobj <- ROCR::prediction(predictions = as.numeric(x = tune.test ),
                            labels      = as.numeric(x = svm.real))
svm.perform <- ROCR::performance(prediction.obj = svm.predobj,
                                 measure        = 'tpr',
                                 x.measure      = 'fpr')
plot(x = svm.perform, main = 'ROC curve')
MLmetrics::F1_Score(y_pred   = tune.test ,
                    y_true   = svm.real,
                    positive = "Yes"); pROC::auc(response = as.numeric(x = svm.real),
                                                 predictor = as.numeric(x = tune.test))

```



## RandomForest
### RandomForest sem imputação
#### K-fold cross validation
```{r, warning=FALSE}
library(caret)
library(randomForest)

# Define the control
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

set.seed(1234)
# Run the model
rf_default <- train(diabetes ~.,
    data = train_without_NAs,
    method = "rf",
    metric = "Accuracy",
    trControl = trControl)
# Print the results
print(rf_default)

```

#### Search best mtry (Number of candidates draw to feed the algorithm)
```{r,warning=FALSE}
set.seed(1234)
tuneGrid <- expand.grid(.mtry = c(1: 10))
rf_mtry <- train(diabetes ~.,
    data = train_without_NAs,
    method = "rf",
    metric = "Accuracy",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    nodesize = 14,
    ntree = 300)
print(rf_mtry)
#The best value of mtry is stored in 
best_mtry <- rf_mtry$bestTune$mtry
best_mtry
```

#### Search best maxnodes (Number of candidates draw to feed the algorithm)
Lets find the maximum amount of terminal nodes in the forest searching for the best maxnodes
```{r}
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = best_mtry)
for (maxnodes in c(5: 15)) {
    set.seed(1234)
    rf_maxnode <- train(diabetes~.,
        data = train_without_NAs,
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        nodesize = 14,
        maxnodes = maxnodes,
        ntree = 300)
    current_iteration <- toString(maxnodes)
    store_maxnode[[current_iteration]] <- rf_maxnode
}
results_mtry <- resamples(store_maxnode)
summary(results_mtry)
```

Lets try again adding more nodes to our maxnodes search
```{r}
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = best_mtry)
for (maxnodes in c(20: 30)) {
    set.seed(1234)
    rf_maxnode <- train(diabetes~.,
        data = train_without_NAs,
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        nodesize = 14,
        maxnodes = maxnodes,
        ntree = 300)
    key <- toString(maxnodes)
    store_maxnode[[key]] <- rf_maxnode
}
results_node <- resamples(store_maxnode)
summary(results_node)
```
Based on these metrics the best maxnodes values should be 20!

#### Search the best ntrees (Number of candidates draw to feed the algorithm)

```{r}
store_maxtrees <- list()
for (ntree in c(250, 300, 350, 400, 450, 500, 550, 600, 800, 1000, 2000)) {
    set.seed(5678)
    rf_maxtrees <- train(diabetes~.,
        data = train_without_NAs,
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tuneGrid,
        trControl = trControl,
        importance = TRUE,
        nodesize = 14,
        maxnodes = 24,
        ntree = ntree)
    key <- toString(ntree)
    store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
summary(results_tree)
```
Based on these metrics the best maxnodes values should be 1000

#### Run RandomForest with without tuning parameters 
```{r, warning=FALSE}
library(randomForest)
library(caret)
set.seed(1234)
learn_rf <- randomForest(diabetes ~., data=train_without_NAs, ntree=1000, proximity=T, importance=T)

pre_rf   <- predict(learn_rf, test_without_NAs[,-9])

cm_rf    <- confusionMatrix(pre_rf, test_without_NAs$diabetes)

cm_rf

```

```{r}

plot(learn_rf, main="Random Forest (Error Rate vs. Number of Trees)")

```



#### Prediction Plot


```{r}

plot(margin(learn_rf,test$diabetes))

```



##### Variance Importance Plot

```{r}

varImpPlot(learn_rf)
varImp(learn_rf)

```


#### Run RandomForest with the final model using tuning
```{r}
set.seed(1234)
learn_rf_final <- train(diabetes~.,
    train_without_NAs,
    method = "rf",
    metric = "Accuracy",
    tuneGrid = tuneGrid,
    trControl = trControl,
    importance = TRUE,
    nodesize = 14,
    ntree = 1000,
    maxnodes = 20)
```

#### Evaluate the final model
```{r}
pre_rf   <- predict(learn_rf_final, test_without_NAs[,-9])

cm_rf    <- confusionMatrix(pre_rf, test_without_NAs$diabetes)

cm_rf
```

#### prepare to plot trees
```{r}
library(dplyr)
library(ggraph)
library(igraph)

tree_func <- function(final_model, 
                      tree_num) {
  
  # get tree by index
  tree <- randomForest::getTree(final_model, 
                                k = tree_num, 
                                labelVar = TRUE) %>%
    tibble::rownames_to_column() %>%
    # make leaf split points to NA, so the 0s won't get plotted
    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))
  
  # prepare data frame for graph
  graph_frame <- data.frame(from = rep(tree$rowname, 2),
                            to = c(tree$`left daughter`, tree$`right daughter`))
  
  # convert to graph and delete the last node that we don't want to plot
  graph <- graph_from_data_frame(graph_frame) %>%
    delete_vertices("0")
  
  # set node labels
  V(graph)$node_label <- gsub("_", " ", as.character(tree$`split var`))
  V(graph)$leaf_label <- as.character(tree$prediction)
  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))
  
  # plot
  plot <- ggraph(graph, 'dendrogram') + 
    theme_bw() +
    geom_edge_link() +
    geom_node_point() +
    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +
    geom_node_label(aes(label = split), vjust = 2, na.rm = TRUE, fill = "white") +
    geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE, 
					repel = TRUE, colour = "white", fontface = "bold", show.legend = FALSE) +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          panel.background = element_blank(),
          plot.background = element_rect(fill = "white"),
          panel.border = element_blank(),
          axis.line = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.title = element_text(size = 14))
  
  return(plot)
}

```


#Plot minimun tree
```{r}
tree_num <- which(learn_rf_final$finalModel$forest$ndbigtree == min(learn_rf_final$finalModel$forest$ndbigtree))
tree_func(final_model = learn_rf_final$finalModel, tree_num)
```

#Plot different trees 
```{r}
tr_min <- tree_func(final_model = learn_rf_final$finalModel, tree_num)
tr_10 <- tree_func(final_model = learn_rf_final$finalModel, 10)
tr_100 <- tree_func(final_model = learn_rf_final$finalModel, 100)
tr_50 <- tree_func(final_model = learn_rf_final$finalModel, 50)
ggarrange(tr_min, tr_50, tr_10,tr_100, 
          labels = c("A", "B", "C", "D"),
          ncol = 2, nrow = 2)
```







